{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data of 39 buildings\n",
    "df = pd.read_csv('./USD_CNY历史数据-2.csv')\n",
    "building_names = df.columns[1:]\n",
    "load = []\n",
    "for name in building_names:\n",
    "    load.append(df[name].values)\n",
    "load = np.array(load)\n",
    "\n",
    "# choose one building\n",
    "data_nor = load[0][::-1] \n",
    "\n",
    "# use max-min normalizaion\n",
    "# maxval = np.max(data_ori)\n",
    "# minval = np.min(data_ori)\n",
    "# data_nor = list(map(lambda x: (x - minval) / (maxval - minval), data_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "ifs = 1000   # input feature size = 1\n",
    "hfs = 30  # hidden feature size = 10\n",
    "pfs = 365   # predict feature size = 1\n",
    "mbs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_pred_navie_dataset(ratio=0.7):\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    \n",
    "    for i in range(len(data_nor)-ifs-pfs):\n",
    "        data_X.append(data_ori[i:i+ifs])\n",
    "        data_Y.append(data_ori[i+ifs:i+ifs+pfs])\n",
    "    \n",
    "    train_size = int(len(data_X) * ratio)\n",
    "    train_r = train_size - (train_size % mbs)\n",
    "    \n",
    "    test_size = len(data_X) - train_r\n",
    "    test_r = test_size - (test_size % mbs) + train_r\n",
    "        \n",
    "    train_X = data_X[: train_r]\n",
    "    train_Y = data_Y[: train_r]\n",
    "    test_X = data_X[train_r : test_r]\n",
    "    test_Y = data_Y[train_r : test_r]\n",
    "    \n",
    "    return [\n",
    "        np.array(train_X).astype(np.float32),\n",
    "        np.array(train_Y).astype(np.float32),\n",
    "        np.array(test_X).astype(np.float32), \n",
    "        np.array(test_Y).astype(np.float32)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((355, 1000), (355, 365))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_X, train_Y, test_X, test_Y] = multi_pred_navie_dataset(0.2)\n",
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, input_size=24, hidden_size=30, output_size=24, num_layer=1):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layer)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.hidden2pred = nn.Linear(hidden_size, output_size) \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = autograd.Variable(torch.zeros(self.num_layer, mbs, self.hidden_size)).cuda()\n",
    "        c0 = autograd.Variable(torch.zeros(self.num_layer, mbs, self.hidden_size)).cuda()\n",
    "        return (h0, c0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        x, y, z = lstm_out.size()\n",
    "        output = self.hidden2pred(lstm_out.view(x * y, z))\n",
    "        return output.view(x, y, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_X, train_Y, test_X, test_Y] = multi_pred_navie_dataset(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, input_size=1000, hidden_size=30, output_size=365, num_layer=1):\n",
    "        super(lstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layer)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.hidden2pred = nn.Linear(hidden_size, output_size) \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = autograd.Variable(torch.zeros(self.num_layer, mbs, self.hidden_size)).cuda()\n",
    "        c0 = autograd.Variable(torch.zeros(self.num_layer, mbs, self.hidden_size)).cuda()\n",
    "        return (h0, c0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x, self.hidden)\n",
    "        x, y, z = lstm_out.size()\n",
    "        output = self.hidden2pred(lstm_out.view(x * y, z))\n",
    "        return output.view(x, y, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(train_X).view(-1, mbs, ifs).cuda()\n",
    "train_y = torch.from_numpy(train_Y).view(-1, mbs,pfs).cuda()\n",
    "test_x = torch.from_numpy(test_X).view(-1, mbs, ifs).cuda()\n",
    "test_y = torch.from_numpy(test_Y).view(-1, mbs, pfs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 2.73708\n",
      "Epoch: 100, Loss: 0.01502\n",
      "Epoch: 150, Loss: 0.00837\n",
      "Epoch: 200, Loss: 0.00830\n",
      "Epoch: 250, Loss: 0.00830\n",
      "Epoch: 300, Loss: 0.00830\n",
      "Epoch: 350, Loss: 0.00830\n",
      "Epoch: 400, Loss: 0.00830\n",
      "Epoch: 450, Loss: 0.00830\n",
      "Epoch: 500, Loss: 0.00830\n",
      "Epoch: 550, Loss: 0.00830\n",
      "Epoch: 600, Loss: 0.00829\n",
      "Epoch: 650, Loss: 0.00829\n",
      "Epoch: 700, Loss: 0.00829\n",
      "Epoch: 750, Loss: 0.00829\n",
      "Epoch: 800, Loss: 0.00829\n",
      "CPU times: user 7.33 s, sys: 46.9 ms, total: 7.38 s\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in range(800):\n",
    "    var_x = Variable(train_x)\n",
    "    var_y = Variable(train_y)\n",
    "    \n",
    "    out = model(var_x)\n",
    "    loss = criterion(out, var_y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (e + 1) % 50 == 0:\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(e + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1424, 1, 365])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.eval()\n",
    "pred_tensor = model(test_x)\n",
    "pred_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 365])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_newest = pred_tensor[1423]\n",
    "pred_newest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.1869, 6.1861, 6.1852, 6.1845, 6.1834, 6.1827, 6.1819, 6.1810, 6.1801,\n",
       "         6.1793, 6.1783, 6.1777, 6.1768, 6.1761, 6.1750, 6.1742, 6.1734, 6.1726,\n",
       "         6.1717, 6.1708, 6.1701, 6.1694, 6.1686, 6.1679, 6.1670, 6.1663, 6.1656,\n",
       "         6.1650, 6.1644, 6.1637, 6.1629, 6.1622, 6.1616, 6.1609, 6.1603, 6.1599,\n",
       "         6.1591, 6.1586, 6.1578, 6.1573, 6.1567, 6.1563, 6.1557, 6.1553, 6.1548,\n",
       "         6.1544, 6.1541, 6.1538, 6.1537, 6.1531, 6.1528, 6.1525, 6.1523, 6.1520,\n",
       "         6.1515, 6.1515, 6.1509, 6.1508, 6.1507, 6.1505, 6.1503, 6.1503, 6.1502,\n",
       "         6.1501, 6.1501, 6.1499, 6.1498, 6.1499, 6.1499, 6.1496, 6.1497, 6.1497,\n",
       "         6.1496, 6.1496, 6.1495, 6.1493, 6.1493, 6.1494, 6.1493, 6.1493, 6.1493,\n",
       "         6.1494, 6.1493, 6.1492, 6.1492, 6.1490, 6.1491, 6.1492, 6.1492, 6.1493,\n",
       "         6.1494, 6.1496, 6.1496, 6.1497, 6.1495, 6.1494, 6.1496, 6.1496, 6.1495,\n",
       "         6.1496, 6.1497, 6.1495, 6.1497, 6.1495, 6.1496, 6.1497, 6.1497, 6.1497,\n",
       "         6.1498, 6.1499, 6.1500, 6.1501, 6.1502, 6.1502, 6.1504, 6.1504, 6.1506,\n",
       "         6.1505, 6.1507, 6.1506, 6.1507, 6.1506, 6.1506, 6.1506, 6.1506, 6.1506,\n",
       "         6.1506, 6.1505, 6.1506, 6.1504, 6.1506, 6.1504, 6.1506, 6.1504, 6.1503,\n",
       "         6.1501, 6.1499, 6.1500, 6.1500, 6.1498, 6.1500, 6.1500, 6.1497, 6.1496,\n",
       "         6.1496, 6.1495, 6.1495, 6.1495, 6.1493, 6.1495, 6.1495, 6.1493, 6.1493,\n",
       "         6.1492, 6.1492, 6.1490, 6.1489, 6.1489, 6.1486, 6.1485, 6.1485, 6.1483,\n",
       "         6.1481, 6.1480, 6.1478, 6.1478, 6.1476, 6.1475, 6.1473, 6.1472, 6.1469,\n",
       "         6.1470, 6.1466, 6.1467, 6.1467, 6.1465, 6.1463, 6.1461, 6.1460, 6.1460,\n",
       "         6.1460, 6.1458, 6.1459, 6.1456, 6.1456, 6.1457, 6.1455, 6.1455, 6.1453,\n",
       "         6.1454, 6.1453, 6.1454, 6.1454, 6.1453, 6.1454, 6.1451, 6.1453, 6.1452,\n",
       "         6.1453, 6.1451, 6.1453, 6.1454, 6.1453, 6.1455, 6.1454, 6.1453, 6.1453,\n",
       "         6.1453, 6.1453, 6.1454, 6.1454, 6.1453, 6.1453, 6.1454, 6.1452, 6.1452,\n",
       "         6.1453, 6.1452, 6.1451, 6.1450, 6.1451, 6.1449, 6.1449, 6.1448, 6.1447,\n",
       "         6.1448, 6.1446, 6.1447, 6.1446, 6.1443, 6.1443, 6.1444, 6.1443, 6.1444,\n",
       "         6.1444, 6.1444, 6.1442, 6.1442, 6.1443, 6.1443, 6.1444, 6.1444, 6.1442,\n",
       "         6.1444, 6.1444, 6.1445, 6.1444, 6.1446, 6.1447, 6.1449, 6.1450, 6.1450,\n",
       "         6.1454, 6.1454, 6.1456, 6.1459, 6.1461, 6.1463, 6.1467, 6.1469, 6.1472,\n",
       "         6.1476, 6.1478, 6.1481, 6.1482, 6.1486, 6.1488, 6.1490, 6.1494, 6.1494,\n",
       "         6.1498, 6.1501, 6.1504, 6.1507, 6.1508, 6.1509, 6.1511, 6.1515, 6.1516,\n",
       "         6.1520, 6.1522, 6.1525, 6.1529, 6.1532, 6.1535, 6.1538, 6.1542, 6.1546,\n",
       "         6.1550, 6.1554, 6.1559, 6.1562, 6.1563, 6.1568, 6.1571, 6.1574, 6.1577,\n",
       "         6.1583, 6.1585, 6.1589, 6.1592, 6.1594, 6.1600, 6.1605, 6.1608, 6.1614,\n",
       "         6.1619, 6.1622, 6.1628, 6.1632, 6.1638, 6.1644, 6.1649, 6.1654, 6.1659,\n",
       "         6.1664, 6.1667, 6.1673, 6.1678, 6.1682, 6.1684, 6.1689, 6.1692, 6.1696,\n",
       "         6.1698, 6.1701, 6.1705, 6.1709, 6.1712, 6.1714, 6.1719, 6.1721, 6.1724,\n",
       "         6.1726, 6.1728, 6.1732, 6.1734, 6.1737, 6.1740, 6.1746, 6.1748, 6.1751,\n",
       "         6.1754, 6.1760, 6.1764, 6.1766, 6.1770, 6.1773, 6.1777, 6.1782, 6.1783,\n",
       "         6.1789, 6.1793, 6.1797, 6.1798, 6.1804, 6.1805, 6.1811, 6.1814, 6.1820,\n",
       "         6.1823, 6.1829, 6.1831, 6.1836, 6.1841]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tensor_p = pred_newest[0].cpu().detach().numpy()\n",
    "fileout = pd.DataFrame(data=pred_tensor_p)\n",
    "fileout.to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133225,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_p = np.array(pred_tensor_p).reshape(-1)\n",
    "pred_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = torch.from_numpy(test_Y).view(-1, mbs, pfs).cuda()\n",
    "test_y_p = [test_y[i].cpu().detach().numpy() for i in range(365)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133225,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_p = np.array(test_y_p).reshape(-1)\n",
    "real_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00729416 0.00722716 0.00701714 0.00734658 0.00686804 0.00714543\n",
      "  0.00718439 0.00725807 0.00698497 0.00710954 0.00695031 0.00733246\n",
      "  0.00712837 0.00747232 0.00707078 0.00709731 0.0072021  0.00726989\n",
      "  0.00700724 0.00682982 0.00695054 0.00722532 0.00704058 0.00727871\n",
      "  0.00699838 0.0069391  0.00691413 0.0072174  0.00753926 0.00757126\n",
      "  0.00717528 0.0072602  0.00729644 0.00715244 0.00719683 0.00773632\n",
      "  0.00740555 0.0075743  0.00741833 0.00753303 0.00771111 0.0078888\n",
      "  0.00764088 0.00805966 0.00778869 0.00773773 0.00770339 0.00782616\n",
      "  0.00819968 0.00763354 0.00779109 0.00774995 0.00814003 0.00813757\n",
      "  0.00777432 0.00830622 0.00773368 0.00797661 0.00831412 0.00801441\n",
      "  0.00801816 0.00811295 0.00801528 0.00786056 0.0080646  0.00768739\n",
      "  0.00776476 0.00802218 0.00813369 0.00765847 0.00805421 0.00810449\n",
      "  0.00814495 0.0080802  0.00810235 0.0078843  0.00800483 0.00820242\n",
      "  0.00809829 0.00810542 0.00805796 0.00832197 0.008138   0.00808374\n",
      "  0.00825085 0.00787699 0.00804628 0.00807192 0.00790316 0.00795869\n",
      "  0.00813815 0.00836223 0.00808329 0.00829503 0.00796052 0.00774926\n",
      "  0.00816895 0.00813488 0.00790497 0.00816163 0.00832962 0.007892\n",
      "  0.00834249 0.00774493 0.00807194 0.00820899 0.00798337 0.00800185\n",
      "  0.00808663 0.0080343  0.00808018 0.00811662 0.00803407 0.00796655\n",
      "  0.00806785 0.00802048 0.00816526 0.00782839 0.0079583  0.00778837\n",
      "  0.00802366 0.0078596  0.00794003 0.00789668 0.00791064 0.00804662\n",
      "  0.00793344 0.00778169 0.00800277 0.00773869 0.00798698 0.00777777\n",
      "  0.0083177  0.00801828 0.0080045  0.00778224 0.0076745  0.00800341\n",
      "  0.00807233 0.00782085 0.00847589 0.0085823  0.00804353 0.00806632\n",
      "  0.00814148 0.00796244 0.00811278 0.00799845 0.00771687 0.00827378\n",
      "  0.00824815 0.00806164 0.008102   0.00818351 0.00835625 0.00819925\n",
      "  0.00816224 0.00839347 0.00797721 0.00808629 0.00826752 0.00822075\n",
      "  0.00813736 0.00822319 0.00806894 0.00825453 0.00830738 0.00834889\n",
      "  0.00820745 0.00846615 0.00811905 0.0086094  0.00808796 0.0085046\n",
      "  0.00870257 0.00841675 0.00819695 0.00806133 0.00803606 0.00813843\n",
      "  0.00838606 0.00824449 0.00857398 0.00816305 0.00815762 0.00858483\n",
      "  0.00849794 0.00857528 0.00815532 0.0085035  0.00833477 0.00847528\n",
      "  0.00858205 0.00835921 0.00848388 0.00800681 0.00834499 0.00829817\n",
      "  0.00852909 0.00810274 0.00848339 0.0085367  0.00828967 0.0085688\n",
      "  0.00826435 0.00799745 0.00805711 0.0079886  0.0081643  0.00821335\n",
      "  0.00842053 0.00825878 0.00828566 0.00859702 0.00827662 0.00836184\n",
      "  0.00855531 0.00839732 0.00818985 0.008183   0.00843531 0.00825824\n",
      "  0.00840352 0.00823512 0.0083228  0.00856542 0.00833264 0.00882057\n",
      "  0.00857927 0.00809439 0.00812685 0.0081647  0.00812271 0.00836162\n",
      "  0.00848694 0.00855822 0.00811489 0.00815787 0.00847032 0.00837706\n",
      "  0.00872012 0.00869948 0.00817752 0.00855039 0.00832823 0.00855763\n",
      "  0.00816161 0.00850889 0.00858017 0.0085515  0.00846297 0.00818331\n",
      "  0.00851995 0.0080685  0.00814902 0.00845873 0.00827871 0.00800016\n",
      "  0.00824937 0.00808922 0.0079798  0.00839312 0.00829216 0.00826124\n",
      "  0.00796175 0.00830935 0.00818828 0.00806723 0.00833017 0.00787274\n",
      "  0.00815301 0.00805439 0.00814193 0.00832941 0.00817867 0.00785704\n",
      "  0.00780044 0.00813628 0.00763602 0.00795386 0.00776479 0.00804488\n",
      "  0.00807504 0.00798054 0.00778072 0.00762323 0.00783897 0.00778704\n",
      "  0.00776933 0.00788089 0.0080341  0.00786259 0.0073635  0.00765642\n",
      "  0.0076065  0.00753675 0.00737528 0.0078804  0.00762756 0.00770268\n",
      "  0.00736793 0.00705379 0.00742393 0.00753832 0.00737642 0.00764486\n",
      "  0.00761249 0.00729016 0.00751777 0.00715631 0.00738107 0.00749537\n",
      "  0.00747253 0.00749149 0.00739762 0.00754069 0.00716528 0.0075058\n",
      "  0.00740758 0.00742376 0.0070568  0.00745337 0.00727943 0.00744245\n",
      "  0.00722456 0.00720997 0.00717662 0.00730371 0.00725572 0.00716594\n",
      "  0.00763991 0.00727716 0.00722884 0.00706622 0.0068254  0.00704777\n",
      "  0.00686233 0.00683648 0.0067204  0.00729143 0.00698722 0.00694735\n",
      "  0.00695339 0.00742967 0.0074553  0.00706748 0.00734516 0.00721365\n",
      "  0.00716517 0.00735442 0.00680561 0.00739847 0.00740497 0.00742164\n",
      "  0.00678119 0.00739543 0.00679968 0.00710605 0.00692937 0.00729681\n",
      "  0.00704534 0.00737948 0.00685088 0.00703731 0.00710016]]\n"
     ]
    }
   ],
   "source": [
    "diff = []\n",
    "for i in range(364):\n",
    "    diff.append((real_p[i] - pred_p[i])**2)\n",
    "print(sum(diff) / len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.0747, 6.0686, 6.0617, ..., 6.6716, 6.7074, 6.722 ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2-D, but have shapes (365,) and (365, 1, 365)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fa75d88e02b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predition visualization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2840\u001b[0m     return gca().plot(\n\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             raise ValueError(f\"x and y can be no greater than 2-D, but have \"\n\u001b[0m\u001b[1;32m    403\u001b[0m                              f\"shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y can be no greater than 2-D, but have shapes (365,) and (365, 1, 365)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.title('predition visualization')\n",
    "plt.plot(pred_p, 'r', label='prediction')\n",
    "plt.plot(real_p, 'b', label='real')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pred_p.reshape(-1, 24)\n",
    "r = real_p.reshape(-1, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psum, rsum, pmax, rmax, pmin, rmin = [], [], [], [], [], []\n",
    "\n",
    "for i in range(p.shape[0]):\n",
    "    psum.append(sum(p[i]))\n",
    "    rsum.append(sum(r[i]))\n",
    "    \n",
    "    pmax.append(max(p[i]))\n",
    "    rmax.append(max(r[i]))\n",
    "    \n",
    "    pmin.append(min(p[i]))\n",
    "    rmin.append(min(r[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.title('psum & rsum')\n",
    "plt.plot(psum, 'r', label='pred')\n",
    "plt.plot(rsum, 'b', label='real')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.title('pmax & rmax')\n",
    "plt.plot(pmax, 'r', label='pred')\n",
    "plt.plot(rmax, 'b', label='real')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.title('pmin & rmin')\n",
    "plt.plot(pmin, 'r', label='pred')\n",
    "plt.plot(rmin, 'b', label='real')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumdiff = 0\n",
    "for i in range(len(psum)):\n",
    "    sumdiff += (psum[i] - rsum[i]) ** 2\n",
    "    \n",
    "sumdiff / len(psum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdiff = 0\n",
    "for i in range(len(pmax)):\n",
    "    maxdiff += (pmax[i] - rmax[i]) ** 2\n",
    "    \n",
    "maxdiff / len(pmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindiff = 0\n",
    "for i in range(len(pmax)):\n",
    "    mindiff += (pmin[i] - rmin[i]) ** 2\n",
    "    \n",
    "mindiff / len(pmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
